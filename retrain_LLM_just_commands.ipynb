{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Guía paso a paso para re-entrenar un LLM con tus propios datos\n"
      ],
      "metadata": {
        "id": "OYH4Y91iEwM-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3zkH1I3FbogH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5ea96a-7ab6-420c-97cc-64c2c2447e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-rpg'...\n",
            "remote: Enumerating objects: 154, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 154 (delta 83), reused 97 (delta 36), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (154/154), 2.16 MiB | 2.09 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ralogon/llm-rpg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r llm-rpg/requirements.txt\n"
      ],
      "metadata": {
        "id": "tb8_G6iagxqK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6a3187d-e4eb-41b2-e956-23e97e7fd5d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes==0.39.1 (from -r llm-rpg/requirements.txt (line 1))\n",
            "  Downloading bitsandbytes-0.39.1-py3-none-any.whl (97.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.13.1 (from -r llm-rpg/requirements.txt (line 2))\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting memory-profiler==0.61.0 (from -r llm-rpg/requirements.txt (line 3))\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Collecting pdfplumber==0.9.0 (from -r llm-rpg/requirements.txt (line 4))\n",
            "  Downloading pdfplumber-0.9.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.3.0 (from -r llm-rpg/requirements.txt (line 5))\n",
            "  Downloading peft-0.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.28.1 (from -r llm-rpg/requirements.txt (line 6))\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (4.65.0)\n",
            "Collecting xxhash (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler==0.61.0->-r llm-rpg/requirements.txt (line 3)) (5.9.5)\n",
            "Collecting pdfminer.six==20221105 (from pdfplumber==0.9.0->-r llm-rpg/requirements.txt (line 4))\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow>=9.1 (from pdfplumber==0.9.0->-r llm-rpg/requirements.txt (line 4))\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Wand>=0.6.10 (from pdfplumber==0.9.0->-r llm-rpg/requirements.txt (line 4))\n",
            "  Downloading Wand-0.6.11-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.6/143.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0->-r llm-rpg/requirements.txt (line 5)) (2.0.1+cu118)\n",
            "Collecting accelerate (from peft==0.3.0->-r llm-rpg/requirements.txt (line 5))\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1->-r llm-rpg/requirements.txt (line 6)) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1->-r llm-rpg/requirements.txt (line 6)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1->-r llm-rpg/requirements.txt (line 6))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber==0.9.0->-r llm-rpg/requirements.txt (line 4)) (2.0.12)\n",
            "Collecting cryptography>=36.0.0 (from pdfminer.six==20221105->pdfplumber==0.9.0->-r llm-rpg/requirements.txt (line 4))\n",
            "  Downloading cryptography-41.0.1-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0->-r llm-rpg/requirements.txt (line 5)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0->-r llm-rpg/requirements.txt (line 5)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0->-r llm-rpg/requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0->-r llm-rpg/requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0->-r llm-rpg/requirements.txt (line 5)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0->-r llm-rpg/requirements.txt (line 5)) (16.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber==0.9.0->-r llm-rpg/requirements.txt (line 4)) (1.15.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.13.1->-r llm-rpg/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.3.0->-r llm-rpg/requirements.txt (line 5)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.3.0->-r llm-rpg/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber==0.9.0->-r llm-rpg/requirements.txt (line 4)) (2.21)\n",
            "Installing collected packages: Wand, tokenizers, bitsandbytes, xxhash, Pillow, memory-profiler, dill, multiprocess, huggingface-hub, cryptography, transformers, pdfminer.six, pdfplumber, datasets, accelerate, peft\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "Successfully installed Pillow-9.5.0 Wand-0.6.11 accelerate-0.20.3 bitsandbytes-0.39.1 cryptography-41.0.1 datasets-2.13.1 dill-0.3.6 huggingface-hub-0.15.1 memory-profiler-0.61.0 multiprocess-0.70.14 pdfminer.six-20221105 pdfplumber-0.9.0 peft-0.3.0 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtención de los datos"
      ],
      "metadata": {
        "id": "cGpsvkKfhLHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python llm-rpg/llm/extract_text.py"
      ],
      "metadata": {
        "id": "kgxEDSAOj5ys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c236106f-61bb-4061-8933-54433361e5bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Start extracting pages from https://web.seducoahuila.gob.mx/biblioweb/upload/J.R.R.%20Tolkien%20La%20Comunidad%20del%20anillo%20I.pdf\n",
            "INFO:__main__:Finished extracting texts from https://web.seducoahuila.gob.mx/biblioweb/upload/J.R.R.%20Tolkien%20La%20Comunidad%20del%20anillo%20I.pdf\n",
            "INFO:__main__:Start writing to /content/llm-rpg/llm/data/extracted_text.json\n",
            "0it [00:00, ?it/s]\n",
            "1it [00:00,  2.29it/s]\n",
            "2it [00:00,  3.37it/s]\n",
            "3it [00:00,  3.84it/s]\n",
            "4it [00:01,  3.93it/s]\n",
            "5it [00:01,  4.23it/s]\n",
            "6it [00:01,  4.23it/s]\n",
            "8it [00:01,  5.75it/s]\n",
            "10it [00:01,  6.93it/s]\n",
            "12it [00:02,  8.92it/s]\n",
            "14it [00:02,  9.55it/s]\n",
            "18it [00:02, 12.75it/s]\n",
            "20it [00:02, 11.05it/s]\n",
            "22it [00:02, 10.70it/s]\n",
            "24it [00:03, 10.09it/s]\n",
            "26it [00:03, 10.12it/s]\n",
            "28it [00:03,  9.71it/s]\n",
            "30it [00:03, 10.16it/s]\n",
            "32it [00:03,  9.76it/s]\n",
            "34it [00:04, 11.06it/s]\n",
            "36it [00:04, 10.83it/s]\n",
            "38it [00:04, 10.25it/s]\n",
            "40it [00:04, 10.17it/s]\n",
            "42it [00:04, 10.29it/s]\n",
            "44it [00:05, 10.29it/s]\n",
            "46it [00:05, 10.06it/s]\n",
            "48it [00:05,  9.43it/s]\n",
            "49it [00:05,  9.50it/s]\n",
            "50it [00:05,  8.99it/s]\n",
            "51it [00:05,  9.01it/s]\n",
            "52it [00:05,  9.15it/s]\n",
            "54it [00:06,  9.80it/s]\n",
            "55it [00:06,  8.16it/s]\n",
            "56it [00:06,  6.99it/s]\n",
            "57it [00:06,  6.54it/s]\n",
            "58it [00:06,  6.26it/s]\n",
            "59it [00:07,  5.48it/s]\n",
            "60it [00:07,  5.49it/s]\n",
            "61it [00:07,  5.63it/s]\n",
            "62it [00:07,  5.49it/s]\n",
            "63it [00:07,  5.55it/s]\n",
            "64it [00:08,  5.69it/s]\n",
            "65it [00:08,  5.37it/s]\n",
            "66it [00:08,  5.93it/s]\n",
            "67it [00:08,  5.93it/s]\n",
            "68it [00:08,  5.48it/s]\n",
            "69it [00:08,  5.28it/s]\n",
            "70it [00:09,  5.16it/s]\n",
            "72it [00:09,  7.14it/s]\n",
            "73it [00:09,  6.93it/s]\n",
            "74it [00:09,  6.64it/s]\n",
            "75it [00:09,  6.06it/s]\n",
            "76it [00:10,  6.08it/s]\n",
            "77it [00:10,  6.10it/s]\n",
            "78it [00:10,  6.09it/s]\n",
            "79it [00:10,  6.34it/s]\n",
            "81it [00:10,  7.82it/s]\n",
            "83it [00:10, 10.12it/s]\n",
            "85it [00:10, 10.04it/s]\n",
            "87it [00:11, 10.27it/s]\n",
            "89it [00:11,  9.99it/s]\n",
            "91it [00:11, 10.27it/s]\n",
            "93it [00:11, 11.86it/s]\n",
            "95it [00:11, 10.87it/s]\n",
            "97it [00:12, 10.57it/s]\n",
            "99it [00:12, 10.12it/s]\n",
            "101it [00:12, 10.20it/s]\n",
            "103it [00:12, 10.19it/s]\n",
            "105it [00:12, 11.42it/s]\n",
            "107it [00:13, 11.19it/s]\n",
            "109it [00:13, 10.53it/s]\n",
            "111it [00:13, 10.06it/s]\n",
            "113it [00:13,  9.53it/s]\n",
            "115it [00:13, 10.11it/s]\n",
            "117it [00:14,  9.99it/s]\n",
            "119it [00:14,  9.66it/s]\n",
            "121it [00:14, 10.32it/s]\n",
            "123it [00:14, 10.06it/s]\n",
            "125it [00:14,  9.92it/s]\n",
            "127it [00:15,  9.96it/s]\n",
            "129it [00:15, 10.31it/s]\n",
            "131it [00:15,  9.91it/s]\n",
            "133it [00:15,  9.92it/s]\n",
            "134it [00:15,  9.74it/s]\n",
            "136it [00:15, 10.74it/s]\n",
            "138it [00:16, 11.00it/s]\n",
            "140it [00:16, 10.98it/s]\n",
            "142it [00:16, 10.17it/s]\n",
            "144it [00:16, 10.31it/s]\n",
            "146it [00:16, 10.12it/s]\n",
            "148it [00:17, 10.16it/s]\n",
            "150it [00:17, 11.47it/s]\n",
            "152it [00:17, 10.62it/s]\n",
            "154it [00:17, 10.37it/s]\n",
            "156it [00:17,  9.92it/s]\n",
            "158it [00:18, 10.28it/s]\n",
            "160it [00:18,  9.69it/s]\n",
            "162it [00:18, 10.01it/s]\n",
            "164it [00:18, 11.01it/s]\n",
            "166it [00:18, 10.33it/s]\n",
            "168it [00:18, 11.00it/s]\n",
            "170it [00:19,  9.98it/s]\n",
            "172it [00:19, 10.23it/s]\n",
            "174it [00:19,  9.60it/s]\n",
            "176it [00:19, 10.31it/s]\n",
            "178it [00:20, 10.39it/s]\n",
            "180it [00:20,  9.89it/s]\n",
            "182it [00:20,  9.69it/s]\n",
            "100% 183/183 [00:20<00:00,  9.03it/s]\n",
            "183it [00:20,  8.92it/s]\n",
            "INFO:__main__:Finished writing to /content/llm-rpg/llm/data/extracted_text.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-procesado de los datos"
      ],
      "metadata": {
        "id": "4Sz36Muw4e5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA3ll7vJrW5D",
        "outputId": "9a70f0f5-1301-493d-dd68-d6664bb86b69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python llm-rpg/llm/prepare_dataset.py"
      ],
      "metadata": {
        "id": "j0vXy306qDp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15302fc1-3fc3-4d60-af09-35477f1beea4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading (…)okenizer_config.json: 100% 222/222 [00:00<00:00, 1.12MB/s]\n",
            "Downloading tokenizer.json: 100% 14.5M/14.5M [00:00<00:00, 369MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 85.0/85.0 [00:00<00:00, 544kB/s]\n",
            "INFO:__main__:Start preparing dataset from /content/llm-rpg/llm/data/extracted_text.json\n",
            "INFO:__main__:The tokenized dataset is composed of 147 elements, each one composed of 2048 tokens.\n",
            "INFO:__main__:The training dataset is composed of 132 elements, the test dataset is composed of 15 elements.\n",
            "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
            "Pushing dataset shards to the dataset hub:   0% 0/1 [00:00<?, ?it/s]\n",
            "Creating parquet from Arrow format: 100% 1/1 [00:00<00:00, 64.07ba/s]\n",
            "\n",
            "Upload 1 LFS files:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Upload 1 LFS files: 100% 1/1 [00:02<00:00,  2.48s/it]\n",
            "Pushing dataset shards to the dataset hub: 100% 1/1 [00:03<00:00,  3.44s/it]\n",
            "Deleting unused files from dataset repository: 100% 1/1 [00:00<00:00,  3.61it/s]\n",
            "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
            "Pushing dataset shards to the dataset hub:   0% 0/1 [00:00<?, ?it/s]\n",
            "Creating parquet from Arrow format: 100% 1/1 [00:00<00:00, 365.04ba/s]\n",
            "\n",
            "Upload 1 LFS files:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Upload 1 LFS files: 100% 1/1 [00:01<00:00,  1.75s/it]\n",
            "Pushing dataset shards to the dataset hub: 100% 1/1 [00:02<00:00,  2.59s/it]\n",
            "Deleting unused files from dataset repository: 100% 1/1 [00:00<00:00,  3.56it/s]\n",
            "Downloading metadata: 100% 426/426 [00:00<00:00, 3.17MB/s]\n",
            "INFO:__main__:Preparing dataset finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "lCSGY-envhIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python llm-rpg/llm/training.py"
      ],
      "metadata": {
        "id": "sDWwCWuOqIfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac645ba5-9b76-4d4e-dd0b-3c7b15ba5f31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-28 11:10:19.235131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-b2dwmcgo4hmu --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Downloading (…)lve/main/config.json: 100% 693/693 [00:00<00:00, 4.29MB/s]\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
            "Downloading pytorch_model.bin: 100% 6.01G/6.01G [06:39<00:00, 15.0MB/s]\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
            "INFO:__main__:Model trainable parameters:\n",
            " trainable params: 4915200 || all params: 3007472640 || trainable%: 0.1634329082375293\n",
            "Downloading readme: 100% 430/430 [00:00<00:00, 2.92MB/s]\n",
            "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/ralogon___parquet/ralogon--llm-tolkien-spanish-10406318b6c9d8f9/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
            "Downloading data files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/473k [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   4% 19.5k/473k [00:00<00:04, 93.3kB/s]\u001b[A\n",
            "Downloading data:  11% 53.2k/473k [00:00<00:03, 134kB/s] \u001b[A\n",
            "Downloading data:  30% 143k/473k [00:00<00:01, 269kB/s] \u001b[A\n",
            "Downloading data: 100% 473k/473k [00:00<00:00, 564kB/s]\n",
            "Downloading data files:  50% 1/2 [00:02<00:02,  2.58s/it]\n",
            "Downloading data:   0% 0.00/69.3k [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  27% 18.4k/69.3k [00:00<00:00, 88.8kB/s]\u001b[A\n",
            "Downloading data: 100% 69.3k/69.3k [00:00<00:00, 166kB/s]\n",
            "Downloading data files: 100% 2/2 [00:05<00:00,  2.69s/it]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1827.19it/s]\n",
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/ralogon___parquet/ralogon--llm-tolkien-spanish-10406318b6c9d8f9/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 196.33it/s]\n",
            "INFO:__main__:Train dataset downloaded:\n",
            " Dataset({\n",
            "    features: ['input_ids'],\n",
            "    num_rows: 132\n",
            "})\n",
            "INFO:__main__:Number of tokens for the training: 270336\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "  0% 0/396 [00:00<?, ?it/s]You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 3.0381, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
            "{'loss': 3.1267, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n",
            "{'loss': 3.0833, 'learning_rate': 6e-06, 'epoch': 0.02}\n",
            "{'loss': 3.2064, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.03}\n",
            "{'loss': 3.37, 'learning_rate': 1e-05, 'epoch': 0.04}\n",
            "{'loss': 3.0925, 'learning_rate': 1.2e-05, 'epoch': 0.05}\n",
            "{'loss': 3.0504, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.05}\n",
            "{'loss': 3.2027, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.06}\n",
            "{'loss': 3.2419, 'learning_rate': 1.8e-05, 'epoch': 0.07}\n",
            "{'loss': 3.3723, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
            "{'loss': 3.0333, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.08}\n",
            "{'loss': 3.2198, 'learning_rate': 2.4e-05, 'epoch': 0.09}\n",
            "{'loss': 3.2184, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.1}\n",
            "{'loss': 3.0656, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.11}\n",
            "{'loss': 3.2392, 'learning_rate': 3e-05, 'epoch': 0.11}\n",
            "{'loss': 3.1139, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.12}\n",
            "{'loss': 3.2496, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.13}\n",
            "{'loss': 3.093, 'learning_rate': 3.6e-05, 'epoch': 0.14}\n",
            "{'loss': 3.2358, 'learning_rate': 3.8e-05, 'epoch': 0.14}\n",
            "{'loss': 3.0577, 'learning_rate': 4e-05, 'epoch': 0.15}\n",
            "{'loss': 3.0296, 'learning_rate': 4.2e-05, 'epoch': 0.16}\n",
            "{'loss': 3.1163, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.17}\n",
            "{'loss': 3.1987, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.17}\n",
            "{'loss': 3.1889, 'learning_rate': 4.8e-05, 'epoch': 0.18}\n",
            "{'loss': 2.9743, 'learning_rate': 5e-05, 'epoch': 0.19}\n",
            "{'loss': 3.1584, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.2}\n",
            "{'loss': 3.1937, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.2}\n",
            "{'loss': 3.0963, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.21}\n",
            "{'loss': 2.9714, 'learning_rate': 5.8e-05, 'epoch': 0.22}\n",
            "{'loss': 3.2277, 'learning_rate': 6e-05, 'epoch': 0.23}\n",
            "{'loss': 3.013, 'learning_rate': 6.2e-05, 'epoch': 0.23}\n",
            "{'loss': 3.0234, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.24}\n",
            "{'loss': 2.8906, 'learning_rate': 6.6e-05, 'epoch': 0.25}\n",
            "{'loss': 3.1458, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.26}\n",
            "{'loss': 3.2249, 'learning_rate': 7e-05, 'epoch': 0.27}\n",
            "{'loss': 2.8588, 'learning_rate': 7.2e-05, 'epoch': 0.27}\n",
            "{'loss': 3.163, 'learning_rate': 7.4e-05, 'epoch': 0.28}\n",
            "{'loss': 3.0765, 'learning_rate': 7.6e-05, 'epoch': 0.29}\n",
            "{'loss': 2.988, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.3}\n",
            "{'loss': 2.9879, 'learning_rate': 8e-05, 'epoch': 0.3}\n",
            "{'loss': 2.9353, 'learning_rate': 8.2e-05, 'epoch': 0.31}\n",
            "{'loss': 2.9657, 'learning_rate': 8.4e-05, 'epoch': 0.32}\n",
            "{'loss': 3.0063, 'learning_rate': 8.6e-05, 'epoch': 0.33}\n",
            "{'loss': 2.9747, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.33}\n",
            "{'loss': 2.9619, 'learning_rate': 9e-05, 'epoch': 0.34}\n",
            "{'loss': 2.967, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.35}\n",
            "{'loss': 2.8407, 'learning_rate': 9.4e-05, 'epoch': 0.36}\n",
            "{'loss': 2.8812, 'learning_rate': 9.6e-05, 'epoch': 0.36}\n",
            "{'loss': 2.8377, 'learning_rate': 9.8e-05, 'epoch': 0.37}\n",
            "{'loss': 2.8213, 'learning_rate': 0.0001, 'epoch': 0.38}\n",
            "{'loss': 2.9442, 'learning_rate': 0.00010200000000000001, 'epoch': 0.39}\n",
            "{'loss': 2.8901, 'learning_rate': 0.00010400000000000001, 'epoch': 0.39}\n",
            "{'loss': 2.7963, 'learning_rate': 0.00010600000000000002, 'epoch': 0.4}\n",
            "{'loss': 2.7659, 'learning_rate': 0.00010800000000000001, 'epoch': 0.41}\n",
            "{'loss': 2.8972, 'learning_rate': 0.00011000000000000002, 'epoch': 0.42}\n",
            "{'loss': 2.8921, 'learning_rate': 0.00011200000000000001, 'epoch': 0.42}\n",
            "{'loss': 2.9145, 'learning_rate': 0.00011399999999999999, 'epoch': 0.43}\n",
            "{'loss': 2.7806, 'learning_rate': 0.000116, 'epoch': 0.44}\n",
            "{'loss': 2.8417, 'learning_rate': 0.000118, 'epoch': 0.45}\n",
            "{'loss': 2.9945, 'learning_rate': 0.00012, 'epoch': 0.45}\n",
            "{'loss': 2.8362, 'learning_rate': 0.000122, 'epoch': 0.46}\n",
            "{'loss': 2.8017, 'learning_rate': 0.000124, 'epoch': 0.47}\n",
            "{'loss': 2.7574, 'learning_rate': 0.000126, 'epoch': 0.48}\n",
            "{'loss': 2.7825, 'learning_rate': 0.00012800000000000002, 'epoch': 0.48}\n",
            "{'loss': 2.7539, 'learning_rate': 0.00013000000000000002, 'epoch': 0.49}\n",
            "{'loss': 3.0262, 'learning_rate': 0.000132, 'epoch': 0.5}\n",
            "{'loss': 2.7802, 'learning_rate': 0.000134, 'epoch': 0.51}\n",
            "{'loss': 2.8249, 'learning_rate': 0.00013600000000000003, 'epoch': 0.52}\n",
            "{'loss': 2.9954, 'learning_rate': 0.000138, 'epoch': 0.52}\n",
            "{'loss': 2.8065, 'learning_rate': 0.00014, 'epoch': 0.53}\n",
            "{'loss': 2.8726, 'learning_rate': 0.000142, 'epoch': 0.54}\n",
            "{'loss': 2.8461, 'learning_rate': 0.000144, 'epoch': 0.55}\n",
            "{'loss': 2.7515, 'learning_rate': 0.000146, 'epoch': 0.55}\n",
            "{'loss': 2.7912, 'learning_rate': 0.000148, 'epoch': 0.56}\n",
            "{'loss': 2.9132, 'learning_rate': 0.00015000000000000001, 'epoch': 0.57}\n",
            "{'loss': 2.7944, 'learning_rate': 0.000152, 'epoch': 0.58}\n",
            "{'loss': 2.7666, 'learning_rate': 0.000154, 'epoch': 0.58}\n",
            "{'loss': 2.6351, 'learning_rate': 0.00015600000000000002, 'epoch': 0.59}\n",
            "{'loss': 2.9193, 'learning_rate': 0.00015800000000000002, 'epoch': 0.6}\n",
            "{'loss': 2.9375, 'learning_rate': 0.00016, 'epoch': 0.61}\n",
            "{'loss': 2.9224, 'learning_rate': 0.000162, 'epoch': 0.61}\n",
            "{'loss': 2.8033, 'learning_rate': 0.000164, 'epoch': 0.62}\n",
            "{'loss': 2.8677, 'learning_rate': 0.000166, 'epoch': 0.63}\n",
            "{'loss': 2.814, 'learning_rate': 0.000168, 'epoch': 0.64}\n",
            "{'loss': 2.6776, 'learning_rate': 0.00017, 'epoch': 0.64}\n",
            "{'loss': 2.9048, 'learning_rate': 0.000172, 'epoch': 0.65}\n",
            "{'loss': 2.8174, 'learning_rate': 0.000174, 'epoch': 0.66}\n",
            "{'loss': 2.7082, 'learning_rate': 0.00017600000000000002, 'epoch': 0.67}\n",
            "{'loss': 2.6272, 'learning_rate': 0.00017800000000000002, 'epoch': 0.67}\n",
            "{'loss': 2.8979, 'learning_rate': 0.00018, 'epoch': 0.68}\n",
            "{'loss': 2.6776, 'learning_rate': 0.000182, 'epoch': 0.69}\n",
            "{'loss': 2.9394, 'learning_rate': 0.00018400000000000003, 'epoch': 0.7}\n",
            "{'loss': 2.756, 'learning_rate': 0.00018600000000000002, 'epoch': 0.7}\n",
            "{'loss': 3.1436, 'learning_rate': 0.000188, 'epoch': 0.71}\n",
            "{'loss': 2.6856, 'learning_rate': 0.00019, 'epoch': 0.72}\n",
            "{'loss': 2.7136, 'learning_rate': 0.000192, 'epoch': 0.73}\n",
            "{'loss': 2.9687, 'learning_rate': 0.000194, 'epoch': 0.73}\n",
            "{'loss': 2.7144, 'learning_rate': 0.000196, 'epoch': 0.74}\n",
            "{'loss': 2.7393, 'learning_rate': 0.00019800000000000002, 'epoch': 0.75}\n",
            "{'loss': 2.7443, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
            "{'loss': 2.7176, 'learning_rate': 0.00019932432432432433, 'epoch': 0.77}\n",
            "{'loss': 2.8043, 'learning_rate': 0.00019864864864864865, 'epoch': 0.77}\n",
            "{'loss': 2.8027, 'learning_rate': 0.000197972972972973, 'epoch': 0.78}\n",
            "{'loss': 2.8847, 'learning_rate': 0.0001972972972972973, 'epoch': 0.79}\n",
            "{'loss': 3.0245, 'learning_rate': 0.00019662162162162162, 'epoch': 0.8}\n",
            "{'loss': 2.6581, 'learning_rate': 0.00019594594594594594, 'epoch': 0.8}\n",
            "{'loss': 2.8152, 'learning_rate': 0.00019527027027027027, 'epoch': 0.81}\n",
            "{'loss': 2.7424, 'learning_rate': 0.00019459459459459462, 'epoch': 0.82}\n",
            "{'loss': 2.6626, 'learning_rate': 0.00019391891891891894, 'epoch': 0.83}\n",
            "{'loss': 3.0201, 'learning_rate': 0.00019324324324324326, 'epoch': 0.83}\n",
            "{'loss': 2.8529, 'learning_rate': 0.00019256756756756758, 'epoch': 0.84}\n",
            "{'loss': 2.8093, 'learning_rate': 0.0001918918918918919, 'epoch': 0.85}\n",
            "{'loss': 2.8372, 'learning_rate': 0.0001912162162162162, 'epoch': 0.86}\n",
            "{'loss': 2.638, 'learning_rate': 0.00019054054054054055, 'epoch': 0.86}\n",
            "{'loss': 3.0286, 'learning_rate': 0.00018986486486486487, 'epoch': 0.87}\n",
            "{'loss': 2.7379, 'learning_rate': 0.0001891891891891892, 'epoch': 0.88}\n",
            "{'loss': 2.857, 'learning_rate': 0.00018851351351351352, 'epoch': 0.89}\n",
            "{'loss': 2.69, 'learning_rate': 0.00018783783783783784, 'epoch': 0.89}\n",
            "{'loss': 2.8695, 'learning_rate': 0.0001871621621621622, 'epoch': 0.9}\n",
            "{'loss': 2.7676, 'learning_rate': 0.0001864864864864865, 'epoch': 0.91}\n",
            "{'loss': 2.8031, 'learning_rate': 0.0001858108108108108, 'epoch': 0.92}\n",
            "{'loss': 2.8098, 'learning_rate': 0.00018513513513513513, 'epoch': 0.92}\n",
            "{'loss': 2.9087, 'learning_rate': 0.00018445945945945946, 'epoch': 0.93}\n",
            "{'loss': 2.6877, 'learning_rate': 0.0001837837837837838, 'epoch': 0.94}\n",
            "{'loss': 2.6255, 'learning_rate': 0.00018310810810810813, 'epoch': 0.95}\n",
            "{'loss': 2.5688, 'learning_rate': 0.00018243243243243245, 'epoch': 0.95}\n",
            "{'loss': 2.6724, 'learning_rate': 0.00018175675675675677, 'epoch': 0.96}\n",
            "{'loss': 2.6369, 'learning_rate': 0.0001810810810810811, 'epoch': 0.97}\n",
            "{'loss': 2.5618, 'learning_rate': 0.0001804054054054054, 'epoch': 0.98}\n",
            "{'loss': 2.93, 'learning_rate': 0.00017972972972972974, 'epoch': 0.98}\n",
            "{'loss': 2.6495, 'learning_rate': 0.00017905405405405406, 'epoch': 0.99}\n",
            "{'loss': 2.6478, 'learning_rate': 0.00017837837837837839, 'epoch': 1.0}\n",
            "{'loss': 2.731, 'learning_rate': 0.0001777027027027027, 'epoch': 1.01}\n",
            "{'loss': 2.9314, 'learning_rate': 0.00017702702702702703, 'epoch': 1.02}\n",
            "{'loss': 2.6687, 'learning_rate': 0.00017635135135135138, 'epoch': 1.02}\n",
            "{'loss': 2.4558, 'learning_rate': 0.00017567567567567568, 'epoch': 1.03}\n",
            "{'loss': 2.6584, 'learning_rate': 0.000175, 'epoch': 1.04}\n",
            "{'loss': 2.655, 'learning_rate': 0.00017432432432432432, 'epoch': 1.05}\n",
            "{'loss': 2.5916, 'learning_rate': 0.00017364864864864864, 'epoch': 1.05}\n",
            "{'loss': 2.7391, 'learning_rate': 0.000172972972972973, 'epoch': 1.06}\n",
            "{'loss': 2.7343, 'learning_rate': 0.00017229729729729732, 'epoch': 1.07}\n",
            "{'loss': 2.6427, 'learning_rate': 0.00017162162162162164, 'epoch': 1.08}\n",
            "{'loss': 2.5432, 'learning_rate': 0.00017094594594594596, 'epoch': 1.08}\n",
            "{'loss': 2.5191, 'learning_rate': 0.00017027027027027028, 'epoch': 1.09}\n",
            "{'loss': 2.7269, 'learning_rate': 0.0001695945945945946, 'epoch': 1.1}\n",
            "{'loss': 2.5158, 'learning_rate': 0.00016891891891891893, 'epoch': 1.11}\n",
            "{'loss': 2.5772, 'learning_rate': 0.00016824324324324325, 'epoch': 1.11}\n",
            "{'loss': 2.5759, 'learning_rate': 0.00016756756756756757, 'epoch': 1.12}\n",
            "{'loss': 2.941, 'learning_rate': 0.0001668918918918919, 'epoch': 1.13}\n",
            "{'loss': 2.6939, 'learning_rate': 0.00016621621621621622, 'epoch': 1.14}\n",
            "{'loss': 2.7001, 'learning_rate': 0.00016554054054054057, 'epoch': 1.14}\n",
            "{'loss': 2.7444, 'learning_rate': 0.00016486486486486486, 'epoch': 1.15}\n",
            "{'loss': 2.5255, 'learning_rate': 0.00016418918918918919, 'epoch': 1.16}\n",
            "{'loss': 2.5587, 'learning_rate': 0.0001635135135135135, 'epoch': 1.17}\n",
            "{'loss': 2.8935, 'learning_rate': 0.00016283783783783783, 'epoch': 1.17}\n",
            "{'loss': 2.3983, 'learning_rate': 0.00016216216216216218, 'epoch': 1.18}\n",
            "{'loss': 2.5248, 'learning_rate': 0.0001614864864864865, 'epoch': 1.19}\n",
            "{'loss': 2.5752, 'learning_rate': 0.00016081081081081083, 'epoch': 1.2}\n",
            "{'loss': 2.4969, 'learning_rate': 0.00016013513513513515, 'epoch': 1.2}\n",
            "{'loss': 2.837, 'learning_rate': 0.00015945945945945947, 'epoch': 1.21}\n",
            "{'loss': 2.6652, 'learning_rate': 0.0001587837837837838, 'epoch': 1.22}\n",
            "{'loss': 2.5039, 'learning_rate': 0.00015810810810810812, 'epoch': 1.23}\n",
            "{'loss': 2.7263, 'learning_rate': 0.00015743243243243244, 'epoch': 1.23}\n",
            "{'loss': 2.6786, 'learning_rate': 0.00015675675675675676, 'epoch': 1.24}\n",
            "{'loss': 2.455, 'learning_rate': 0.00015608108108108108, 'epoch': 1.25}\n",
            "{'loss': 2.6135, 'learning_rate': 0.0001554054054054054, 'epoch': 1.26}\n",
            "{'loss': 2.5724, 'learning_rate': 0.00015472972972972976, 'epoch': 1.27}\n",
            "{'loss': 2.561, 'learning_rate': 0.00015405405405405405, 'epoch': 1.27}\n",
            "{'loss': 2.5126, 'learning_rate': 0.00015337837837837837, 'epoch': 1.28}\n",
            "{'loss': 2.7959, 'learning_rate': 0.0001527027027027027, 'epoch': 1.29}\n",
            "{'loss': 2.6085, 'learning_rate': 0.00015202702702702702, 'epoch': 1.3}\n",
            "{'loss': 2.6954, 'learning_rate': 0.00015135135135135137, 'epoch': 1.3}\n",
            "{'loss': 2.6052, 'learning_rate': 0.0001506756756756757, 'epoch': 1.31}\n",
            "{'loss': 2.8119, 'learning_rate': 0.00015000000000000001, 'epoch': 1.32}\n",
            "{'loss': 2.3826, 'learning_rate': 0.00014932432432432434, 'epoch': 1.33}\n",
            "{'loss': 2.6239, 'learning_rate': 0.00014864864864864866, 'epoch': 1.33}\n",
            "{'loss': 2.5688, 'learning_rate': 0.00014797297297297298, 'epoch': 1.34}\n",
            "{'loss': 2.5182, 'learning_rate': 0.0001472972972972973, 'epoch': 1.35}\n",
            "{'loss': 2.7128, 'learning_rate': 0.00014662162162162163, 'epoch': 1.36}\n",
            "{'loss': 2.5502, 'learning_rate': 0.00014594594594594595, 'epoch': 1.36}\n",
            "{'loss': 2.4084, 'learning_rate': 0.00014527027027027027, 'epoch': 1.37}\n",
            "{'loss': 2.4218, 'learning_rate': 0.00014459459459459462, 'epoch': 1.38}\n",
            "{'loss': 2.6889, 'learning_rate': 0.00014391891891891894, 'epoch': 1.39}\n",
            "{'loss': 2.5946, 'learning_rate': 0.00014324324324324324, 'epoch': 1.39}\n",
            "{'loss': 2.4853, 'learning_rate': 0.00014256756756756756, 'epoch': 1.4}\n",
            "{'loss': 2.7504, 'learning_rate': 0.00014189189189189188, 'epoch': 1.41}\n",
            "{'loss': 2.8164, 'learning_rate': 0.0001412162162162162, 'epoch': 1.42}\n",
            "{'loss': 2.5464, 'learning_rate': 0.00014054054054054056, 'epoch': 1.42}\n",
            "{'loss': 2.6445, 'learning_rate': 0.00013986486486486488, 'epoch': 1.43}\n",
            "{'loss': 2.3555, 'learning_rate': 0.0001391891891891892, 'epoch': 1.44}\n",
            "{'loss': 2.6032, 'learning_rate': 0.00013851351351351352, 'epoch': 1.45}\n",
            "{'loss': 2.5867, 'learning_rate': 0.00013783783783783785, 'epoch': 1.45}\n",
            "{'loss': 2.4175, 'learning_rate': 0.00013716216216216217, 'epoch': 1.46}\n",
            "{'loss': 2.9083, 'learning_rate': 0.0001364864864864865, 'epoch': 1.47}\n",
            "{'loss': 2.683, 'learning_rate': 0.00013581081081081081, 'epoch': 1.48}\n",
            "{'loss': 2.5411, 'learning_rate': 0.00013513513513513514, 'epoch': 1.48}\n",
            "{'loss': 2.5783, 'learning_rate': 0.00013445945945945946, 'epoch': 1.49}\n",
            "{'loss': 2.5433, 'learning_rate': 0.0001337837837837838, 'epoch': 1.5}\n",
            "{'loss': 2.768, 'learning_rate': 0.00013310810810810813, 'epoch': 1.51}\n",
            "{'loss': 2.4329, 'learning_rate': 0.00013243243243243243, 'epoch': 1.52}\n",
            "{'loss': 2.7675, 'learning_rate': 0.00013175675675675675, 'epoch': 1.52}\n",
            "{'loss': 2.7302, 'learning_rate': 0.00013108108108108107, 'epoch': 1.53}\n",
            "{'loss': 2.4401, 'learning_rate': 0.0001304054054054054, 'epoch': 1.54}\n",
            "{'loss': 2.4868, 'learning_rate': 0.00012972972972972974, 'epoch': 1.55}\n",
            "{'loss': 2.5551, 'learning_rate': 0.00012905405405405407, 'epoch': 1.55}\n",
            "{'loss': 2.3718, 'learning_rate': 0.0001283783783783784, 'epoch': 1.56}\n",
            "{'loss': 2.4955, 'learning_rate': 0.0001277027027027027, 'epoch': 1.57}\n",
            "{'loss': 2.5569, 'learning_rate': 0.00012702702702702703, 'epoch': 1.58}\n",
            "{'loss': 2.4688, 'learning_rate': 0.00012635135135135136, 'epoch': 1.58}\n",
            "{'loss': 2.5921, 'learning_rate': 0.00012567567567567568, 'epoch': 1.59}\n",
            "{'loss': 2.5355, 'learning_rate': 0.000125, 'epoch': 1.6}\n",
            "{'loss': 2.3324, 'learning_rate': 0.00012432432432432433, 'epoch': 1.61}\n",
            "{'loss': 2.6407, 'learning_rate': 0.00012364864864864865, 'epoch': 1.61}\n",
            "{'loss': 2.4635, 'learning_rate': 0.000122972972972973, 'epoch': 1.62}\n",
            "{'loss': 2.7685, 'learning_rate': 0.00012229729729729732, 'epoch': 1.63}\n",
            "{'loss': 2.5044, 'learning_rate': 0.00012162162162162163, 'epoch': 1.64}\n",
            "{'loss': 2.4764, 'learning_rate': 0.00012094594594594595, 'epoch': 1.64}\n",
            "{'loss': 2.6977, 'learning_rate': 0.00012027027027027027, 'epoch': 1.65}\n",
            "{'loss': 2.606, 'learning_rate': 0.00011959459459459461, 'epoch': 1.66}\n",
            "{'loss': 2.6296, 'learning_rate': 0.00011891891891891893, 'epoch': 1.67}\n",
            "{'loss': 2.6945, 'learning_rate': 0.00011824324324324326, 'epoch': 1.67}\n",
            "{'loss': 2.4718, 'learning_rate': 0.00011756756756756758, 'epoch': 1.68}\n",
            "{'loss': 2.5362, 'learning_rate': 0.00011689189189189189, 'epoch': 1.69}\n",
            "{'loss': 2.3633, 'learning_rate': 0.00011621621621621621, 'epoch': 1.7}\n",
            "{'loss': 2.558, 'learning_rate': 0.00011554054054054056, 'epoch': 1.7}\n",
            "{'loss': 2.3969, 'learning_rate': 0.00011486486486486487, 'epoch': 1.71}\n",
            "{'loss': 2.5268, 'learning_rate': 0.00011418918918918919, 'epoch': 1.72}\n",
            "{'loss': 2.4704, 'learning_rate': 0.00011351351351351351, 'epoch': 1.73}\n",
            "{'loss': 2.6505, 'learning_rate': 0.00011283783783783784, 'epoch': 1.73}\n",
            "{'loss': 2.6773, 'learning_rate': 0.00011216216216216217, 'epoch': 1.74}\n",
            "{'loss': 2.6265, 'learning_rate': 0.0001114864864864865, 'epoch': 1.75}\n",
            "{'loss': 2.6638, 'learning_rate': 0.00011081081081081082, 'epoch': 1.76}\n",
            "{'loss': 2.8397, 'learning_rate': 0.00011013513513513514, 'epoch': 1.77}\n",
            "{'loss': 2.4915, 'learning_rate': 0.00010945945945945946, 'epoch': 1.77}\n",
            "{'loss': 2.4269, 'learning_rate': 0.0001087837837837838, 'epoch': 1.78}\n",
            "{'loss': 2.4845, 'learning_rate': 0.00010810810810810812, 'epoch': 1.79}\n",
            "{'loss': 2.6469, 'learning_rate': 0.00010743243243243244, 'epoch': 1.8}\n",
            "{'loss': 2.7843, 'learning_rate': 0.00010675675675675677, 'epoch': 1.8}\n",
            "{'loss': 2.6124, 'learning_rate': 0.00010608108108108107, 'epoch': 1.81}\n",
            "{'loss': 2.2979, 'learning_rate': 0.0001054054054054054, 'epoch': 1.82}\n",
            "{'loss': 2.9237, 'learning_rate': 0.00010472972972972975, 'epoch': 1.83}\n",
            "{'loss': 2.6174, 'learning_rate': 0.00010405405405405406, 'epoch': 1.83}\n",
            "{'loss': 2.6897, 'learning_rate': 0.00010337837837837838, 'epoch': 1.84}\n",
            "{'loss': 2.3983, 'learning_rate': 0.0001027027027027027, 'epoch': 1.85}\n",
            "{'loss': 2.6503, 'learning_rate': 0.00010202702702702702, 'epoch': 1.86}\n",
            "{'loss': 2.6498, 'learning_rate': 0.00010135135135135136, 'epoch': 1.86}\n",
            "{'loss': 2.698, 'learning_rate': 0.00010067567567567568, 'epoch': 1.87}\n",
            "{'loss': 2.2924, 'learning_rate': 0.0001, 'epoch': 1.88}\n",
            "{'loss': 2.8109, 'learning_rate': 9.932432432432433e-05, 'epoch': 1.89}\n",
            "{'loss': 2.5254, 'learning_rate': 9.864864864864865e-05, 'epoch': 1.89}\n",
            "{'loss': 2.5308, 'learning_rate': 9.797297297297297e-05, 'epoch': 1.9}\n",
            "{'loss': 2.7411, 'learning_rate': 9.729729729729731e-05, 'epoch': 1.91}\n",
            "{'loss': 2.5142, 'learning_rate': 9.662162162162163e-05, 'epoch': 1.92}\n",
            "{'loss': 2.5851, 'learning_rate': 9.594594594594595e-05, 'epoch': 1.92}\n",
            "{'loss': 2.4519, 'learning_rate': 9.527027027027028e-05, 'epoch': 1.93}\n",
            "{'loss': 2.3668, 'learning_rate': 9.45945945945946e-05, 'epoch': 1.94}\n",
            "{'loss': 2.7548, 'learning_rate': 9.391891891891892e-05, 'epoch': 1.95}\n",
            "{'loss': 2.5884, 'learning_rate': 9.324324324324324e-05, 'epoch': 1.95}\n",
            "{'loss': 2.5727, 'learning_rate': 9.256756756756757e-05, 'epoch': 1.96}\n",
            "{'loss': 2.4569, 'learning_rate': 9.18918918918919e-05, 'epoch': 1.97}\n",
            "{'loss': 2.6367, 'learning_rate': 9.121621621621623e-05, 'epoch': 1.98}\n",
            "{'loss': 2.5073, 'learning_rate': 9.054054054054055e-05, 'epoch': 1.98}\n",
            "{'loss': 2.359, 'learning_rate': 8.986486486486487e-05, 'epoch': 1.99}\n",
            "{'loss': 2.4725, 'learning_rate': 8.918918918918919e-05, 'epoch': 2.0}\n",
            "{'loss': 2.2874, 'learning_rate': 8.851351351351352e-05, 'epoch': 2.01}\n",
            "{'loss': 2.5169, 'learning_rate': 8.783783783783784e-05, 'epoch': 2.02}\n",
            "{'loss': 2.3373, 'learning_rate': 8.716216216216216e-05, 'epoch': 2.02}\n",
            "{'loss': 2.5616, 'learning_rate': 8.64864864864865e-05, 'epoch': 2.03}\n",
            "{'loss': 2.5397, 'learning_rate': 8.581081081081082e-05, 'epoch': 2.04}\n",
            "{'loss': 2.5602, 'learning_rate': 8.513513513513514e-05, 'epoch': 2.05}\n",
            "{'loss': 2.3276, 'learning_rate': 8.445945945945946e-05, 'epoch': 2.05}\n",
            "{'loss': 2.3123, 'learning_rate': 8.378378378378379e-05, 'epoch': 2.06}\n",
            "{'loss': 2.4818, 'learning_rate': 8.310810810810811e-05, 'epoch': 2.07}\n",
            "{'loss': 2.2415, 'learning_rate': 8.243243243243243e-05, 'epoch': 2.08}\n",
            "{'loss': 2.3405, 'learning_rate': 8.175675675675675e-05, 'epoch': 2.08}\n",
            "{'loss': 2.318, 'learning_rate': 8.108108108108109e-05, 'epoch': 2.09}\n",
            "{'loss': 2.3331, 'learning_rate': 8.040540540540541e-05, 'epoch': 2.1}\n",
            "{'loss': 2.2549, 'learning_rate': 7.972972972972974e-05, 'epoch': 2.11}\n",
            "{'loss': 2.4575, 'learning_rate': 7.905405405405406e-05, 'epoch': 2.11}\n",
            "{'loss': 2.3001, 'learning_rate': 7.837837837837838e-05, 'epoch': 2.12}\n",
            "{'loss': 2.436, 'learning_rate': 7.77027027027027e-05, 'epoch': 2.13}\n",
            "{'loss': 2.2222, 'learning_rate': 7.702702702702703e-05, 'epoch': 2.14}\n",
            "{'loss': 2.3254, 'learning_rate': 7.635135135135135e-05, 'epoch': 2.14}\n",
            "{'loss': 2.2293, 'learning_rate': 7.567567567567568e-05, 'epoch': 2.15}\n",
            "{'loss': 2.4566, 'learning_rate': 7.500000000000001e-05, 'epoch': 2.16}\n",
            "{'loss': 2.2958, 'learning_rate': 7.432432432432433e-05, 'epoch': 2.17}\n",
            "{'loss': 2.4707, 'learning_rate': 7.364864864864865e-05, 'epoch': 2.17}\n",
            "{'loss': 2.3376, 'learning_rate': 7.297297297297297e-05, 'epoch': 2.18}\n",
            "{'loss': 2.4055, 'learning_rate': 7.229729729729731e-05, 'epoch': 2.19}\n",
            "{'loss': 2.289, 'learning_rate': 7.162162162162162e-05, 'epoch': 2.2}\n",
            "{'loss': 2.4485, 'learning_rate': 7.094594594594594e-05, 'epoch': 2.2}\n",
            "{'loss': 2.2795, 'learning_rate': 7.027027027027028e-05, 'epoch': 2.21}\n",
            "{'loss': 2.3342, 'learning_rate': 6.95945945945946e-05, 'epoch': 2.22}\n",
            "{'loss': 2.64, 'learning_rate': 6.891891891891892e-05, 'epoch': 2.23}\n",
            "{'loss': 2.2857, 'learning_rate': 6.824324324324325e-05, 'epoch': 2.23}\n",
            "{'loss': 2.1696, 'learning_rate': 6.756756756756757e-05, 'epoch': 2.24}\n",
            "{'loss': 2.6515, 'learning_rate': 6.68918918918919e-05, 'epoch': 2.25}\n",
            "{'loss': 2.0951, 'learning_rate': 6.621621621621621e-05, 'epoch': 2.26}\n",
            "{'loss': 2.212, 'learning_rate': 6.554054054054054e-05, 'epoch': 2.27}\n",
            "{'loss': 2.4992, 'learning_rate': 6.486486486486487e-05, 'epoch': 2.27}\n",
            "{'loss': 2.4692, 'learning_rate': 6.41891891891892e-05, 'epoch': 2.28}\n",
            "{'loss': 2.5268, 'learning_rate': 6.351351351351352e-05, 'epoch': 2.29}\n",
            "{'loss': 2.4423, 'learning_rate': 6.283783783783784e-05, 'epoch': 2.3}\n",
            "{'loss': 2.2658, 'learning_rate': 6.216216216216216e-05, 'epoch': 2.3}\n",
            "{'loss': 2.4118, 'learning_rate': 6.14864864864865e-05, 'epoch': 2.31}\n",
            "{'loss': 2.3515, 'learning_rate': 6.0810810810810814e-05, 'epoch': 2.32}\n",
            "{'loss': 2.4996, 'learning_rate': 6.013513513513514e-05, 'epoch': 2.33}\n",
            "{'loss': 2.4008, 'learning_rate': 5.9459459459459466e-05, 'epoch': 2.33}\n",
            "{'loss': 2.57, 'learning_rate': 5.878378378378379e-05, 'epoch': 2.34}\n",
            "{'loss': 2.2351, 'learning_rate': 5.8108108108108105e-05, 'epoch': 2.35}\n",
            "{'loss': 2.3359, 'learning_rate': 5.7432432432432434e-05, 'epoch': 2.36}\n",
            "{'loss': 2.4136, 'learning_rate': 5.6756756756756757e-05, 'epoch': 2.36}\n",
            "{'loss': 2.4871, 'learning_rate': 5.6081081081081086e-05, 'epoch': 2.37}\n",
            "{'loss': 2.4114, 'learning_rate': 5.540540540540541e-05, 'epoch': 2.38}\n",
            "{'loss': 2.3818, 'learning_rate': 5.472972972972973e-05, 'epoch': 2.39}\n",
            "{'loss': 2.2538, 'learning_rate': 5.405405405405406e-05, 'epoch': 2.39}\n",
            "{'loss': 2.1721, 'learning_rate': 5.337837837837838e-05, 'epoch': 2.4}\n",
            "{'loss': 2.1716, 'learning_rate': 5.27027027027027e-05, 'epoch': 2.41}\n",
            "{'loss': 2.2234, 'learning_rate': 5.202702702702703e-05, 'epoch': 2.42}\n",
            "{'loss': 2.6556, 'learning_rate': 5.135135135135135e-05, 'epoch': 2.42}\n",
            "{'loss': 2.377, 'learning_rate': 5.067567567567568e-05, 'epoch': 2.43}\n",
            "{'loss': 2.3784, 'learning_rate': 5e-05, 'epoch': 2.44}\n",
            "{'loss': 2.0902, 'learning_rate': 4.9324324324324325e-05, 'epoch': 2.45}\n",
            "{'loss': 2.2236, 'learning_rate': 4.8648648648648654e-05, 'epoch': 2.45}\n",
            "{'loss': 2.6029, 'learning_rate': 4.797297297297298e-05, 'epoch': 2.46}\n",
            "{'loss': 2.2474, 'learning_rate': 4.72972972972973e-05, 'epoch': 2.47}\n",
            "{'loss': 2.28, 'learning_rate': 4.662162162162162e-05, 'epoch': 2.48}\n",
            "{'loss': 2.3464, 'learning_rate': 4.594594594594595e-05, 'epoch': 2.48}\n",
            "{'loss': 2.4095, 'learning_rate': 4.5270270270270274e-05, 'epoch': 2.49}\n",
            "{'loss': 2.2507, 'learning_rate': 4.4594594594594596e-05, 'epoch': 2.5}\n",
            "{'loss': 2.211, 'learning_rate': 4.391891891891892e-05, 'epoch': 2.51}\n",
            "{'loss': 2.3604, 'learning_rate': 4.324324324324325e-05, 'epoch': 2.52}\n",
            "{'loss': 2.2164, 'learning_rate': 4.256756756756757e-05, 'epoch': 2.52}\n",
            "{'loss': 2.2957, 'learning_rate': 4.189189189189189e-05, 'epoch': 2.53}\n",
            "{'loss': 2.2501, 'learning_rate': 4.1216216216216216e-05, 'epoch': 2.54}\n",
            "{'loss': 2.3897, 'learning_rate': 4.0540540540540545e-05, 'epoch': 2.55}\n",
            "{'loss': 2.2021, 'learning_rate': 3.986486486486487e-05, 'epoch': 2.55}\n",
            "{'loss': 2.4947, 'learning_rate': 3.918918918918919e-05, 'epoch': 2.56}\n",
            "{'loss': 2.2272, 'learning_rate': 3.851351351351351e-05, 'epoch': 2.57}\n",
            "{'loss': 2.2057, 'learning_rate': 3.783783783783784e-05, 'epoch': 2.58}\n",
            "{'loss': 2.1878, 'learning_rate': 3.7162162162162165e-05, 'epoch': 2.58}\n",
            "{'loss': 2.2041, 'learning_rate': 3.648648648648649e-05, 'epoch': 2.59}\n",
            "{'loss': 2.6016, 'learning_rate': 3.581081081081081e-05, 'epoch': 2.6}\n",
            "{'loss': 2.1478, 'learning_rate': 3.513513513513514e-05, 'epoch': 2.61}\n",
            "{'loss': 2.4511, 'learning_rate': 3.445945945945946e-05, 'epoch': 2.61}\n",
            "{'loss': 2.2114, 'learning_rate': 3.3783783783783784e-05, 'epoch': 2.62}\n",
            "{'loss': 2.331, 'learning_rate': 3.310810810810811e-05, 'epoch': 2.63}\n",
            "{'loss': 2.3046, 'learning_rate': 3.2432432432432436e-05, 'epoch': 2.64}\n",
            "{'loss': 2.3316, 'learning_rate': 3.175675675675676e-05, 'epoch': 2.64}\n",
            "{'loss': 2.3218, 'learning_rate': 3.108108108108108e-05, 'epoch': 2.65}\n",
            "{'loss': 2.4443, 'learning_rate': 3.0405405405405407e-05, 'epoch': 2.66}\n",
            "{'loss': 2.4035, 'learning_rate': 2.9729729729729733e-05, 'epoch': 2.67}\n",
            "{'loss': 2.3918, 'learning_rate': 2.9054054054054052e-05, 'epoch': 2.67}\n",
            "{'loss': 2.3248, 'learning_rate': 2.8378378378378378e-05, 'epoch': 2.68}\n",
            "{'loss': 2.3891, 'learning_rate': 2.7702702702702704e-05, 'epoch': 2.69}\n",
            "{'loss': 2.5612, 'learning_rate': 2.702702702702703e-05, 'epoch': 2.7}\n",
            "{'loss': 2.7045, 'learning_rate': 2.635135135135135e-05, 'epoch': 2.7}\n",
            "{'loss': 2.5225, 'learning_rate': 2.5675675675675675e-05, 'epoch': 2.71}\n",
            "{'loss': 2.2116, 'learning_rate': 2.5e-05, 'epoch': 2.72}\n",
            "{'loss': 2.259, 'learning_rate': 2.4324324324324327e-05, 'epoch': 2.73}\n",
            "{'loss': 2.3009, 'learning_rate': 2.364864864864865e-05, 'epoch': 2.73}\n",
            "{'loss': 2.3076, 'learning_rate': 2.2972972972972976e-05, 'epoch': 2.74}\n",
            "{'loss': 2.1546, 'learning_rate': 2.2297297297297298e-05, 'epoch': 2.75}\n",
            "{'loss': 2.2416, 'learning_rate': 2.1621621621621624e-05, 'epoch': 2.76}\n",
            "{'loss': 2.1524, 'learning_rate': 2.0945945945945947e-05, 'epoch': 2.77}\n",
            "{'loss': 2.4807, 'learning_rate': 2.0270270270270273e-05, 'epoch': 2.77}\n",
            "{'loss': 2.4034, 'learning_rate': 1.9594594594594595e-05, 'epoch': 2.78}\n",
            "{'loss': 2.4389, 'learning_rate': 1.891891891891892e-05, 'epoch': 2.79}\n",
            "{'loss': 2.3119, 'learning_rate': 1.8243243243243244e-05, 'epoch': 2.8}\n",
            "{'loss': 2.5336, 'learning_rate': 1.756756756756757e-05, 'epoch': 2.8}\n",
            "{'loss': 2.2841, 'learning_rate': 1.6891891891891892e-05, 'epoch': 2.81}\n",
            "{'loss': 2.2466, 'learning_rate': 1.6216216216216218e-05, 'epoch': 2.82}\n",
            "{'loss': 2.4141, 'learning_rate': 1.554054054054054e-05, 'epoch': 2.83}\n",
            "{'loss': 2.3374, 'learning_rate': 1.4864864864864867e-05, 'epoch': 2.83}\n",
            "{'loss': 2.2599, 'learning_rate': 1.4189189189189189e-05, 'epoch': 2.84}\n",
            "{'loss': 2.1623, 'learning_rate': 1.3513513513513515e-05, 'epoch': 2.85}\n",
            "{'loss': 2.0969, 'learning_rate': 1.2837837837837838e-05, 'epoch': 2.86}\n",
            "{'loss': 2.3482, 'learning_rate': 1.2162162162162164e-05, 'epoch': 2.86}\n",
            "{'loss': 2.0907, 'learning_rate': 1.1486486486486488e-05, 'epoch': 2.87}\n",
            "{'loss': 2.333, 'learning_rate': 1.0810810810810812e-05, 'epoch': 2.88}\n",
            "{'loss': 2.2553, 'learning_rate': 1.0135135135135136e-05, 'epoch': 2.89}\n",
            "{'loss': 2.1176, 'learning_rate': 9.45945945945946e-06, 'epoch': 2.89}\n",
            "{'loss': 2.5973, 'learning_rate': 8.783783783783785e-06, 'epoch': 2.9}\n",
            "{'loss': 2.2228, 'learning_rate': 8.108108108108109e-06, 'epoch': 2.91}\n",
            "{'loss': 2.2446, 'learning_rate': 7.432432432432433e-06, 'epoch': 2.92}\n",
            "{'loss': 2.3184, 'learning_rate': 6.7567567567567575e-06, 'epoch': 2.92}\n",
            "{'loss': 2.2038, 'learning_rate': 6.081081081081082e-06, 'epoch': 2.93}\n",
            "{'loss': 2.6371, 'learning_rate': 5.405405405405406e-06, 'epoch': 2.94}\n",
            "{'loss': 2.1845, 'learning_rate': 4.72972972972973e-06, 'epoch': 2.95}\n",
            "{'loss': 2.1483, 'learning_rate': 4.0540540540540545e-06, 'epoch': 2.95}\n",
            "{'loss': 2.3974, 'learning_rate': 3.3783783783783788e-06, 'epoch': 2.96}\n",
            "{'loss': 2.4085, 'learning_rate': 2.702702702702703e-06, 'epoch': 2.97}\n",
            "{'loss': 2.2884, 'learning_rate': 2.0270270270270273e-06, 'epoch': 2.98}\n",
            "{'loss': 2.1231, 'learning_rate': 1.3513513513513515e-06, 'epoch': 2.98}\n",
            "{'loss': 2.4225, 'learning_rate': 6.756756756756758e-07, 'epoch': 2.99}\n",
            "{'loss': 2.2572, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "{'train_runtime': 2063.0822, 'train_samples_per_second': 0.192, 'train_steps_per_second': 0.192, 'train_loss': 2.6147278023488596, 'epoch': 3.0}\n",
            "100% 396/396 [34:23<00:00,  5.21s/it]\n",
            "Upload 1 LFS files:   0% 0/1 [00:00<?, ?it/s]\n",
            "adapter_model.bin:   0% 0.00/19.7M [00:00<?, ?B/s]\u001b[A\n",
            "adapter_model.bin:   0% 8.19k/19.7M [00:00<27:22, 12.0kB/s]\u001b[A\n",
            "adapter_model.bin:   1% 115k/19.7M [00:00<02:01, 161kB/s]  \u001b[A\n",
            "adapter_model.bin:   1% 180k/19.7M [00:01<01:35, 204kB/s]\u001b[A\n",
            "adapter_model.bin:   2% 303k/19.7M [00:01<01:01, 314kB/s]\u001b[A\n",
            "adapter_model.bin:   2% 483k/19.7M [00:01<00:40, 471kB/s]\u001b[A\n",
            "adapter_model.bin:   4% 803k/19.7M [00:01<00:24, 773kB/s]\u001b[A\n",
            "adapter_model.bin:   8% 1.57M/19.7M [00:01<00:11, 1.61MB/s]\u001b[A\n",
            "adapter_model.bin:  17% 3.29M/19.7M [00:02<00:04, 3.51MB/s]\u001b[A\n",
            "adapter_model.bin:  29% 5.64M/19.7M [00:02<00:02, 5.62MB/s]\u001b[A\n",
            "adapter_model.bin:  37% 7.32M/19.7M [00:02<00:01, 6.23MB/s]\u001b[A\n",
            "adapter_model.bin:  48% 9.36M/19.7M [00:02<00:01, 7.26MB/s]\u001b[A\n",
            "adapter_model.bin:  67% 13.3M/19.7M [00:03<00:00, 10.3MB/s]\u001b[A\n",
            "adapter_model.bin:  77% 15.1M/19.7M [00:03<00:00, 9.83MB/s]\u001b[A\n",
            "adapter_model.bin: 100% 19.7M/19.7M [00:04<00:00, 4.67MB/s]\n",
            "Upload 1 LFS files: 100% 1/1 [00:04<00:00,  4.58s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtén inferencias utilizando el modelo"
      ],
      "metadata": {
        "id": "P3NS5KG3PbrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python llm-rpg/llm/inference.py --prompt \"Los hobbits estaban muy sorprendidos de ver a su amigo\""
      ],
      "metadata": {
        "id": "esGGkPo0ZaOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09ec8ec-8c92-43e3-b5c0-2861c8f3dbee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-28 12:11:13.106518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-b2dwmcgo4hmu --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "\n",
            "==============================\n",
            "\n",
            "SALIDA DEL MODELO: Los hobbits estaban muy sorprendidos de ver a su amigo Frodo, que había estado ausente tanto tiempo, tan lejos, tan lejos de ellos.</s>\n",
            "\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "095PA35t6hnn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}